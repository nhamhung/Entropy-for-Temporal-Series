{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Rationale\n",
    "\n",
    "ApEn is limited: Whatever process (even white noise) is considered, a short data sequence generates zero values of entropy rate for a large (w.r.t total amount of data) length of pattern $(m)$. Hence, we cannot quantify the regularity of the time series.\n",
    "\n",
    "In other words, embedding the dynamics into a too highly dimensional phase space produces entropy rate estimates equal to 0. This forces Pincus et al. (1993) to fix the pattern lengh (i.e. the embedding dimension m) at very small arbitraty values in order to obtain a reliable entropy rate estimate.\n",
    "\n",
    "- Aim: Propose a new method for quantification of regularity of a process from short data sequences, based on defining a new CCE function and search for its minimum in respect of the pattern length.\n",
    "\n",
    "- Attempt to **distinguish** the **decrease of entropy rate** related to **presence of recurrences** from that related to the **shortness of the data sequence**\n",
    "\n",
    "#### Formula\n",
    "\n",
    "##### 1. Conditional Entropy\n",
    "\n",
    "To take advantage of stationary, normalize the series $\\{X(i)\\}$ to a process with 0 mean and unitary variance $$x(i) = \\frac{X(i) - av[X]}{std[X]}$$\n",
    "\n",
    "From the normalized series, a reconstructed $L-dimensional$ phase space with a delay of reconstruction equal to 1 is obtained by considering $N-L+1$ vectors $x_L(i) = (x(i), x(i-1), ..., x(i-L+1)$. Each vector $x_L(i)$ represents a pattern of L consecutive samples.\n",
    "\n",
    "CE is defined: $$E(L/L-1) = - \\sum_{L-1} p_{L-1} \\sum_{L/L-1} p_{L/L-1} \\log p_{L/L-1}$$\n",
    "\n",
    "where $p_{L-1}$ denotes the joint probability of the pattern $x_{L-1}(i)$ and $p_{L/L-1}$ symbolizes conditional probability of the Lth sample of the pattern $x_L(i)$ given the previous $L-1$ ones.\n",
    "\n",
    "CE can be directly derived from definition of Shannon Entropy (SE) of $x_L(i)$ $$E(L) = - \\sum_L p_L \\log p_L$$\n",
    "\n",
    "CE can be obtained as the variation of the SE w.r.t L: $$E(L/L-1) = E(L) - E(L - 1)$$\n",
    "\n",
    "- SE represents the amount of information needed to specify the point $x_L(i)$ in a L-dimensional phase space.\n",
    "\n",
    "- CE quantifies the variation of information necessary to specify a new state in a one-dimension incremental phase space. Small CE values are obtained when a length L pattern can be almost completely predicted by a length L - 1 pattern.\n",
    "\n",
    "##### 2. Conditional entropy estimate\n",
    "\n",
    "$$E_{hat}(L/L-1) = E_{hat}(L) - E_{hat}(L-1)$$ where $E(L)$ and $E(L-1)$ represent estimate of SE in a L-dimensional and (L-1)-dimensional phase space. $E(L)$ can be estimated by approximating probabilities in $E(L) = - \\sum_L p_L \\log p_L$ with the sample frequencies.\n",
    "\n",
    "In order to calculate, series $\\{x(i)\\}$ is spread on $\\xi$ quantization levels each with amplitude $\\epsilon = (x_{max} - x_{min})/\\xi$, where $(x_{max} - x_{min})$ represents full range of process dynamics.\n",
    "\n",
    "Quantization defines a partition of L-dim phase space in $M$ hypercubes ($M = \\xi ^ L$) of side length $\\epsilon$. Points inside each hypercube are a most $\\epsilon$ difference in distance between their coordinates. Since each point represents a length $L$ pattern, several points in the same hypercube mean identical pattern within a precision of $\\epsilon$. In contrast, when a length-L pattern appears only once, the relevant point is single in a hypercube. Since **estimate of sample frequencies depends on both series length N and on number of quantization levels $\\xi$, $E_{hat}(L/L-1)$** is a function of $L, N, \\xi$.\n",
    "\n",
    "##### 3. Effect of limited number of samples on $E_{hat}(L/L-1)$\n",
    "\n",
    "Limited data introduces a negative bias in $E_{hat}(L/L-1)$. A length $L-1$ pattern may be found only once in data sequence (the relevant point is single in a (L-1)-dimension hypercube). In this case the length L pattern, derived as an extension of length ($L-1$) pattern by adding one value, will also be detected only once (the relevant point will be single in a L-dimensional hypercube).\n",
    "\n",
    "Therefore, the unique appearance of length L pattern is completely predicted by length (L-1) pattern (estimate of conditional probability $p_{L/L-1}$ equals 1). So single points in (L-1)-dimensional phase space give a null contribution to $E_{hat}(L/L-1)$.\n",
    "\n",
    "Also, if the series exhibits random components, number of patterns found only once augments with L and number of points contributing to $E_{hat}(L/L-1)$ decreases more. Hence, for a completely stochastic series such as one with guassian white noise, $E_{hat}(L/L-1)$ tends to zero which give a false impression of determinism (Lower entropy -> more regularity).\n",
    "\n",
    "More formally, bias of $\\hat{E}(L/L-1)$  is clear when considering that $\\hat{E}(L) = \\hat{E}_{single}(L) + \\hat{E}_{not single}(L)$, where $\\hat{E}_{single}(L), \\hat{E}_{not single}(L)$ represent contribution of $\\hat{E}(L)$ by single and not single points (in hypercube).\n",
    "$$\\hat{E}_{single}(L) = perc(L).log(N-L+1)$$ where $perc(L)$ denotes percentage of single points in the L-dim phase space.\n",
    "\n",
    "##### 4. Effect of pattern length on $\\hat{E}(L/L-1)$\n",
    "\n",
    "To limit presence of single points, the series length N should be $\\geq \\xi^{L+1}$ so even in randomly distributed noise, an average of $\\geq 1$ point per hypercube for each phase space up to $L$-dimension. In contrast, when only short data sequences are available, both $L$ and $\\xi$ are fixed to small arbitrary values ($L=2$, $\\xi$ from 4 to 10 as in Pincus et al. 1993).\n",
    "\n",
    "However, when N is small, even after limiting $L, \\xi$, some problems arise. As the growth of single points is fast due to exponential increase in M hypercubes ($M=\\xi^L$), we can get different CE values for small L.\n",
    "\n",
    "##### 5. Corrected conditional entropy\n",
    "\n",
    "Rationale:\n",
    "1. Overcome problem of limited samples (i.e growing percentage of single points as L increases)\n",
    "2. Avoid a-priori selection of embedding dimension\n",
    "\n",
    "Formula:\n",
    "Find minimum of this function:\n",
    "$$CCE(L) = \\hat{E}(L/L-1) + E_c(l)$$\n",
    "\n",
    "CCE is sum of CE and **corrective term**:\n",
    "$$E_c(L) = perc(L).\\hat{E}(1)$$\n",
    "where $perc(L)$ is percentage of single points in the L-dim phase space and $\\hat{E}(1)$ is estimated value of SE for $L=1$.\n",
    "\n",
    "Scale factor $E(1)$ chosen as it represents theoretical CE value of white noise with same probability distribution of considered series.\n",
    "\n",
    "$Perc(L)$ is empirically chosen as single (L-1) patterns give a null contribution to CE and single L-patterns from a few (L-1)-patterns also give no robust contribution to $\\hat{E}(L/L-1)$.\n",
    "\n",
    "Since CCE is sum of 2 terms, the first decreasing and second increasing with L, it exhibits a minimum. CCE min is considered best estimate of CE with limited data.\n",
    "\n",
    "#### Advantages\n",
    "1. Possibility to improve reliability of entropy rate estimation\n",
    "- The proposed correction implicitly states that length L patterns which do appear only once cannot be used to fix prediction rules and reduce entropy rate. In contrast, when repetitive patterns detected in data sequence, no correction made and regularity is recognised. So CCE estimate is a max entropy estimate according to available data. When new data considered by enlarging temporal frame, CCE estimate either remains stable if different patterns are found or decrease toward 0 if patterns previously found only once are recognised to be repetitive -> recognise regularity\n",
    "\n",
    "- Minimisation of CCE function: avoid a-priori selection of embedding dim. Thus, dim L corresponding to CCE minimum is best embedding dimension with so few points. It a compromise between low L which do not permit resolution of complex periodic structures and large L which lead rapidly to statistical insignificance due to series' shortness. So, same process can have different optimal embedding dim while varying data set length.\n",
    "\n",
    "Results of experiment show CCE min measure regularity. However, CCE values depend on series length and number of quantization levels:\n",
    "- Small quantization level: less dependence on length of series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from CCE import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original series: [1 2 3 1 2 3]\n",
      "Normalized series: [-1.11803399  0.          1.11803399 -1.11803399  0.          1.11803399]\n",
      "epsilon: 0.447213595499958\n",
      "partition: [-1.11803399 -0.67082039 -0.2236068   0.2236068   0.67082039  1.11803399]\n",
      "codebook: [-1  0  1  2  3  4  5]\n",
      "Uniform quantification of the time series:\n",
      "quantizations before: [-1  2  4 -1  2  4]\n",
      "quantizations after: [0 2 4 0 2 4]\n",
      "Compose patterns of length 'L':\n",
      "X:\n",
      " [[0. 2. 4. 0. 2. 4.]\n",
      " [2. 4. 0. 2. 4. 0.]\n",
      " [4. 0. 2. 4. 0. 0.]]\n",
      "Eliminate last 'L-1' columns of 'X' since they are not real patterns\n",
      "X after\n",
      ": [[0. 2. 4. 0.]\n",
      " [2. 4. 0. 2.]\n",
      " [4. 0. 2. 4.]]\n",
      "Get the number of repetitions of each pattern by going through columns of 'X':\n",
      "col j of X: [0. 2. 4.]\n",
      "col i (j+1 onwards) of X: [2. 4. 0.]\n",
      "col j of X: [0. 2. 4.]\n",
      "col i (j+1 onwards) of X: [4. 0. 2.]\n",
      "col j of X: [0. 2. 4.]\n",
      "col i (j+1 onwards) of X: [0. 2. 4.]\n",
      "2 columns are equal, set col i to nan\n",
      "col j of X: [2. 4. 0.]\n",
      "col i (j+1 onwards) of X: [4. 0. 2.]\n",
      "col j of X: [2. 4. 0.]\n",
      "col i (j+1 onwards) of X: [nan nan nan]\n",
      "col j of X: [4. 0. 2.]\n",
      "col i (j+1 onwards) of X: [nan nan nan]\n",
      "Number of patterns: [2. 1. 1. 1.]\n",
      "Number of patterns which have appeared only once: 2.0\n",
      "Probability of each pattern\n",
      "p_i [0.5  0.25 0.25]\n",
      "Compute Shannon Entropy: 1.0397207708399179\n"
     ]
    },
    {
     "data": {
      "text/plain": "(array(1.03972077), 2.0)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shannon_entropy_with_comments(np.array([1,2,3,1,2,3]), 3, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Shanon entropy for series with embedding dimension 'L': 1.0397207708399179 , number of unique patterns: 2.0\n",
      "Calculate Shanon entropy for series with embedding dimension 'L-1': 1.0549201679861442 , number of unique patterns: 2.0\n",
      "Conditional entropy: -0.015199397146226312\n"
     ]
    },
    {
     "data": {
      "text/plain": "(-0.015199397146226312, 2.0)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_entropy_with_comments(np.array([1,2,3,1,2,3]), 3, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Ê(1) with 'L=1' which will be used to calculate the corrective term: 1.0986122886681096\n",
      "CCE will be a vector that will contain several CCE values computed:\n",
      "We loop for different value of embedding dimensions 'L':\n",
      "\n",
      "L: 2 \n",
      "\n",
      "First, compute CE for the current embedding dimension:\n",
      "CE: [        nan -0.04369212         nan]\n",
      "uniques: [nan  1. nan]\n",
      "Second, compute the percentage of patterns which are not repeated\n",
      "perc_L: 0.2\n",
      "Ê(1): 1.0986122886681096\n",
      "CCE is CE + corrective term:\n",
      "correct_term: [       nan 0.21972246        nan]\n",
      "CCE: [100.           0.17603034          nan]\n",
      "\n",
      "L: 3 \n",
      "\n",
      "First, compute CE for the current embedding dimension:\n",
      "CE: [        nan -0.04369212 -0.0151994 ]\n",
      "uniques: [nan  1.  2.]\n",
      "Second, compute the percentage of patterns which are not repeated\n",
      "perc_L: 0.5\n",
      "Ê(1): 1.0986122886681096\n",
      "CCE is CE + corrective term:\n",
      "correct_term: [       nan 0.21972246 0.54930614]\n",
      "CCE: [100.           0.17603034   0.53410675]\n",
      "\n",
      "Final CCE: [100.           0.17603034   0.53410675]\n",
      "Get min CCE value: 0.17603033705165655\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.17603033705165655"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_conditional_entropy_with_comments(np.array([1,2,3,1,2,3]), 3, 5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Third element randomized: [1 2 9 1 2 2 1 2 0]\n",
      "More regular array: [1 2 3 1 2 3 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "irregular_array = []\n",
    "for i in range(3):\n",
    "    irregular_array.extend([1,2,random.randint(0,10)])\n",
    "irregular_array = np.array(irregular_array)\n",
    "\n",
    "regular_array = np.tile(np.array([1,2,3]), 3)\n",
    "print(\"Third element randomized:\", irregular_array)\n",
    "print(\"More regular array:\", regular_array)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon entropy for more regular array: 1.0549201679861442\n",
      "Conditional entropy for more regular array: -0.043692120681965374\n",
      "Corrected conditional entropy for more regular array: -0.01641675862934222\n",
      "\n",
      "\n",
      "Shannon entropy: 1.6094379124341005\n",
      "Conditional entropy: -0.18232155679395423\n",
      "Corrected conditional entropy: 1.0325680971551663\n"
     ]
    }
   ],
   "source": [
    "print(\"Shannon entropy for more regular array:\", shannon_entropy(regular_array, 5, 10)[0])\n",
    "print(\"Conditional entropy for more regular array:\", conditional_entropy(regular_array, 5, 10)[0])\n",
    "print(\"Corrected conditional entropy for more regular array:\", corrected_conditional_entropy(regular_array, 5, 10))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Shannon entropy:\", shannon_entropy(irregular_array, 5, 10)[0])\n",
    "print(\"Conditional entropy:\", conditional_entropy(irregular_array, 5, 10)[0])\n",
    "print(\"Corrected conditional entropy:\", corrected_conditional_entropy(irregular_array, 5, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}